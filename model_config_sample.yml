# A configuration file contains the specification of the corpus
# files to be used for training and validation, followed by
# a list of models, possibly a single-element list, that will be
# trained on this corpus.
# The base name of the configuration file without the final file
# extension will be used as the name of the training run, so
# generally a new configuration file with a new name should be
# created for a new training run.

# The specification of either a directory path containing the
# corpus files or a list of files (but not both!) is required.

# corpus_files:
# - foo.txt
# - bar.txt

corpus_dir: /var/corpus

# Validation files used during training can be specified as a list.
# If validation files are not listed, not validation is carried out
# during training.

# validation_files:
# - val_baz.txt

# Training and validation batch size for the training of all models.
# This will be used unless overridden for a particular model.
# If not set, the default values for training and validation batch size
# as set in lstm_model.py will be used.
#  train_batch: 256
#  val_batch: 8192


# Specification of models starts here
models:

- model:

# Optional name. If this is not specified, a name is automatically
# generated for the model which lists is characteristics and
# creation timestamp.
# Either this name, or the automatically generated name is used for
# saving the model during training and when training is completed.
# When resuming training of a previously created model, either
# after a crash or when continuing training on further input,
# train_lstm_object.py looks for a saved model with this file name.
# If such a model is not found, a new model is created, otherwise
# training is resumed.

#  name: foo_model

# Optional log file name. If this is not specified, model creation
# and training are not logged.

  log_file: model.log

# Training and validation batch size for this model. Overrides
# the general setting as specified in the training configuration.

#  train_batch: 256
#  val_batch: 8192

# Path to input and output encoder files to be used by the model
  input_encoder: input_encoder.json
  output_encoder: output_encoder.json

# Width of left and right context window around each target token
  left_context: 15
  right_context: 15

# Embedding dimensions
# This is optional, if embeddings is not specified, the
# output of the input encoder is used to encode sequence elements.
  embedding: 32

# Number of LSTM units, i.e. dimension of the vector representation
# that is generated by the LSTM for the input sequence.
# The value is a list, each item of which represents the number of
# units in an LSTM layer.
  lstm_layers:
  - 384
  - 256

# Settings for dense layers. The value is a list of one set of key-value
# pairs for each dense layer. The number of neurons must be specified
# for each layer. Optionally a dropout ratio can also be specified.
# if no dropout is specified, or it is 0, then no dropout layer is added
# for that dense layer.
  dense_layers:
  - neurons: 512
    dropout: 0
  - neurons: 256

# Optional, default is false.
# If this is true, only the outputs of the final LSTM layer are fed to
# the first dense layer. If it is false, the outputs of all LSTM layers
# are concatenated, and the result is passed to the first dense layer.
  final_only: true
